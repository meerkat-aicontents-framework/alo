{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 3,
>>>>>>> a6f08d97c8653df390409b9d799009f85dce6c7b
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.alo import ALO\n",
    "from src.alo import AssetStructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alo = ALO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alo 시작 전 setting\n",
    "alo.preset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline list 를 가지고 옴\n",
    "pipelines = list(alo.asset_source.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-10-31 02:06:57,646][PROCESS][INFO]: You did not write any << s3_private_key_file >> in the config yaml file. When you wanna get data from s3 storage, \n",
      "                                 you have to write the s3_private_key_file path or set << ACCESS_KEY, SECRET_KEY >> in your os environment. \n",
      "\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,647][PROCESS][INFO]:  Skip loading external data. << /nas001/users/yoonji.suh/tcr_test_20231016/train_multiclass/ >> \n",
      " << train_multiclass >> already exists in << /home/wonjun.sung/0.repo/release/release-1.2/input/ >>. \n",
      " & << get_external_data >> is set as << once >>. \n",
      "\u001b[0m\n"
     ]
    }
   ],
>>>>>>> a6f08d97c8653df390409b9d799009f85dce6c7b
   "source": [
    "# 외부 데이터 가져오기\n",
    "from src.external import external_load_data, external_save_artifacts\n",
    "external_load_data(pipelines[0], alo.external_path, alo.external_path_permission, alo.control['get_external_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-10-31 02:06:57,896][PROCESS][INFO]: Start setting-up << input >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,897][PROCESS][INFO]: << input >> asset had already been created at 2023-10-31 10:51:27.855431\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,897][PROCESS][INFO]: Start setting-up << preprocess >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,897][PROCESS][INFO]: << preprocess >> asset had already been created at 2023-10-31 10:57:30.447709\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,898][PROCESS][INFO]: Start setting-up << sampling >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,898][PROCESS][INFO]: << sampling >> asset had already been created at 2023-10-31 10:47:01.979108\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,898][PROCESS][INFO]: Start setting-up << train >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,898][PROCESS][INFO]: << train >> asset had already been created at 2023-10-31 10:47:02.195108\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,899][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,899][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,899][PROCESS][INFO]: >>> Ignored installing << numpy==1.25.2 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,900][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,900][PROCESS][INFO]: >>> Ignored installing << scikit-learn >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,900][PROCESS][INFO]: >>> Ignored installing << matplotlib >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,901][PROCESS][INFO]: ======================================== Start dependency installation : << input >> \u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,901][PROCESS][INFO]: Start checking existence & installing package - pandas==1.5.3 | Progress: ( 1 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,901][PROCESS][INFO]: [OK] << pandas==1.5.3 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,902][PROCESS][INFO]: ======================================== Start dependency installation : << preprocess >> \u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,902][PROCESS][INFO]: Start checking existence & installing package - category_encoders | Progress: ( 2 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,902][PROCESS][INFO]: [OK] << category_encoders >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,903][PROCESS][INFO]: ======================================== Start dependency installation : << sampling >> \u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,903][PROCESS][INFO]: Start checking existence & installing package - numpy==1.25.2 | Progress: ( 3 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,903][PROCESS][INFO]: [OK] << numpy==1.25.2 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,904][PROCESS][INFO]: Start checking existence & installing package - scikit-learn | Progress: ( 4 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,904][PROCESS][INFO]: [OK] << scikit-learn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,904][PROCESS][INFO]: Start checking existence & installing package - umap-learn | Progress: ( 5 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,905][PROCESS][INFO]: [OK] << umap-learn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,905][PROCESS][INFO]: Start checking existence & installing package - matplotlib | Progress: ( 6 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,905][PROCESS][INFO]: [OK] << matplotlib >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,905][PROCESS][INFO]: ======================================== Start dependency installation : << train >> \u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,906][PROCESS][INFO]: Start checking existence & installing package - seaborn | Progress: ( 7 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,906][PROCESS][INFO]: [OK] << seaborn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,906][PROCESS][INFO]: Start checking existence & installing package - shap | Progress: ( 8 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,907][PROCESS][INFO]: [OK] << shap >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,907][PROCESS][INFO]: Start checking existence & installing package - lightgbm | Progress: ( 9 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,907][PROCESS][INFO]: [OK] << lightgbm >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,907][PROCESS][INFO]: Start checking existence & installing package - catboost | Progress: ( 10 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,908][PROCESS][INFO]: [OK] << catboost >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,909][PROCESS][INFO]: Start checking existence & installing package - ngboost | Progress: ( 11 / 12 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-10-31 02:06:57,909][PROCESS][INFO]: [OK] << ngboost >> already exists\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,909][PROCESS][INFO]: ======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,910][PROCESS][INFO]: Start checking existence & installing package - numpy==1.25.2 --force-reinstall | Progress: ( 12 / 12 total packages ) \u001b[0m\n",
      "\u001b[94m[2023-10-31 02:06:57,910][PROCESS][INFO]: >>> Start installing package - numpy==1.25.2 --force-reinstall\u001b[0m\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "Successfully installed numpy-1.25.2\n",
      "\u001b[94m[2023-10-31 02:07:00,181][PROCESS][INFO]: ======================================== Finish dependency installation \n",
      "\u001b[0m\n"
     ]
    }
   ],
>>>>>>> a6f08d97c8653df390409b9d799009f85dce6c7b
   "source": [
    "# 사용하는 pipeline의 package를 설치\n",
    "# train = 0, infernence = 1을 선택해야 하고 둘다 설치 해야함\n",
    "pipeline = pipelines[0]\n",
    "alo.install_steps(pipeline, alo.control[\"get_asset_source\"])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 data structure 구성\n",
    "envs, args, data, config = {}, {}, {}, {}\n",
    "asset_structure = AssetStructure(envs, args, data, config)\n",
    "# logger init\n",
    "alo.set_proc_logger()"
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 data structure 구성\n",
    "envs, args, data, config = {}, {}, {}, {}\n",
    "asset_structure = AssetStructure(envs, args, data, config)\n",
    "# logger init\n",
    "alo.set_proc_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(step, pipeline, asset_structure):\n",
    "    # 반복되는 작업을 함수로 변환\n",
    "    asset_config = alo.asset_source[pipeline]\n",
    "    return alo.process_asset_step(asset_config[step], step, pipeline, asset_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 진행할 step을 list형식으로 입력하기\n",
    "step = 0\n",
    "# 입력할 args를 변수로 저장\n",
    "asset_structure.args = alo.user_parameters[pipeline][step]['args'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자가 args를 변경해서 사용 가능\n",
    "# input_args[0]['input_path'] = 'test'"
>>>>>>> a6f08d97c8653df390409b9d799009f85dce6c7b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(step, pipeline, asset_structure):\n",
    "    # 반복되는 작업을 함수로 변환\n",
    "    asset_config = alo.asset_source[pipeline]\n",
    "    return alo.process_asset_step(asset_config[step], step, pipeline, asset_structure)\n"
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-31 11:07:08,841][USER][INFO][train_pipeline][input]: >> Load path : ['/home/wonjun.sung/0.repo/release/release-1.2//input/train_multiclass/']\n",
      "[2023-10-31 11:07:08,845][USER][INFO][train_pipeline][input]: >> The file for batch data has been loaded. (File name: /home/wonjun.sung/0.repo/release/release-1.2//input/train_multiclass/iris.csv)\n",
      "[2023-10-31 11:07:08,847][USER][INFO][train_pipeline][input]: ==================== Success loading dataframe ====================\n",
      "[2023-10-31 11:07:08,847][USER][INFO][train_pipeline][input]: >> Drop columns from the input dataframe when set << auto >> mode or specified in the << drop_columns >> in config yaml. (dropped colums:[])\n",
      "[2023-10-31 11:07:08,848][USER][INFO][train_pipeline][input]: >> Start processing ignore columns & drop columns: ['/home/wonjun.sung/0.repo/release/release-1.2//input/train_multiclass/iris.csv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-10-31 02:07:08,840][ASSET][INFO][train_pipeline][input]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-10-31 02:07:08\n",
      "- current step      : input\n",
      "- asset branch.     : tabular_dev\n",
      "- alolib ver.       : 0\n",
      "- alo ver.          : release-1.2\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path'])\n",
      "- load args. keys   : dict_keys(['input_path', 'x_columns', 'use_all_x', 'y_column', 'groupkey_columns', 'drop_columns', 'time_column', 'concat_dataframes', 'encoding'])\n",
      "- load config. keys : dict_keys([])\n",
      "- load data keys    : dict_keys([])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:07:08,849][ASSET][INFO][train_pipeline][input]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-10-31 02:07:08\n",
      "- current step      : input\n",
      "- save config. keys : dict_keys(['data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:07:08,850][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: input\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "asset_structure = run(step, pipeline, asset_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "asset_structure.args = alo.user_parameters[pipeline][step]['args'][0]"
>>>>>>> a6f08d97c8653df390409b9d799009f85dce6c7b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 진행할 step을 list형식으로 입력하기\n",
    "step = 0\n",
    "# 입력할 args를 변수로 저장\n",
    "asset_structure.args = alo.user_parameters[pipeline][step]['args'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자가 args를 변경해서 사용 가능\n",
    "# input_args[0]['input_path'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_structure = run(step, pipeline, asset_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "asset_structure.args = alo.user_parameters[pipeline][step]['args'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 15,
>>>>>>> a6f08d97c8653df390409b9d799009f85dce6c7b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-10-31 02:07:15,813][ASSET][INFO][train_pipeline][preprocess]: Successfully got model path for saving or loading your AI model: \n",
      " /home/wonjun.sung/0.repo/release/release-1.2//.train_artifacts/models/preprocess/\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:07:15,814][ASSET][INFO][train_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-10-31 02:07:15\n",
      "- current step      : preprocess\n",
      "- asset branch.     : tcr_release_dev\n",
      "- alolib ver.       : 0\n",
      "- alo ver.          : release-1.2\n",
      "- load envs. keys   : dict_keys(['project_home', 'pipeline', 'step', 'num_step', 'artifacts', 'alo_version', 'asset_branch', 'interface_mode', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['handling_missing', 'handling_outlier_x', 'handling_scaling_x', 'handling_encoding_x_columns', 'handling_encoding_y_column', 'handling_encoding_x', 'handling_encoding_y', 'limit_encoding_categories', 'handling_imputer_x', 'handling_imputer_y', 'drop_duplicate_time', 'load_train_preprocess', 'handling_downsampling_interval', 'downsampling_method'])\n",
      "- load config. keys : dict_keys(['data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "target column : label Encoder saved : /home/wonjun.sung/0.repo/release/release-1.2//.train_artifacts/models/preprocess/\n",
      "['input_x0_nan', 'input_x1_nan', 'input_x2_nan', 'input_x3_nan'] target_encoded_nan\n",
      "\u001b[94m[2023-10-31 02:07:15,820][ASSET][INFO][train_pipeline][preprocess]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-10-31 02:07:15\n",
      "- current step      : preprocess\n",
      "- save config. keys : dict_keys(['data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns', 'columns_map', 'preprocess'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-10-31 02:07:15,820][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: preprocess\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "asset_structure = run(step, pipeline, asset_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('release-1.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c44a7ce709843662cd524fcc2a8e8e0bd459ed9a4e30f104c2fe1b493331e35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
