{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.alo import ALO\n",
    "from src.alo import AssetStructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alo = ALO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-29 04:12:50,777][PROCESS][INFO]: Successfully loaded << experimental_plan.yaml >> from: \n",
      " /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/config/experimental_plan.yaml\u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:50,815][PROCESS][INFO]: Success versioning up experimental_plan.yaml : 2.0 --> 2.1 (version ref. : compare yaml version)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# alo 시작 전 setting\n",
    "alo.set_proc_logger()\n",
    "alo.preset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_pipeline', 'inference_pipeline']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline list 를 가지고 옴\n",
    "pipelines = list(alo.asset_source.keys())\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m[2023-11-29 04:12:55,020][PROCESS][WARNING]: You did not write any << s3_private_key_file >> in the config yaml file. When you wanna get data from s3 storage, \n",
      "                                 you have to write the s3_private_key_file path or set << AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY >> in your os environment. \n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:55,023][PROCESS][INFO]: Skip loading external data. All the data in the external load data path already exist in << /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/input/ >> equally. \n",
      " : << external_base_dirs >>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 외부 데이터 가져오기\n",
    "from src.external import external_load_data, external_save_artifacts\n",
    "alo.external_load_data(pipelines[0]) # external load data for train_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-29 04:12:58,997][PROCESS][INFO]: Start setting-up << input >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,001][PROCESS][INFO]: << input >> asset had already been created at 2023-11-29 12:55:44.854840\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,002][PROCESS][INFO]: Start setting-up << graph >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,003][PROCESS][INFO]: << graph >> asset had already been created at 2023-11-29 13:04:04.459723\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,005][PROCESS][INFO]: Start setting-up << preprocess >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,006][PROCESS][INFO]: << preprocess >> asset had already been created at 2023-11-29 13:04:26.531775\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,007][PROCESS][INFO]: Start setting-up << sampling >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,007][PROCESS][INFO]: << sampling >> asset had already been created at 2023-11-29 13:04:27.079777\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,008][PROCESS][INFO]: Start setting-up << train >> asset @ << assets >> directory.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,009][PROCESS][INFO]: << train >> asset had already been created at 2023-11-29 13:04:36.443799\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,010][PROCESS][INFO]: >>> Ignored installing << torch==2.0.0 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,010][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,011][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,012][PROCESS][INFO]: >>> Ignored installing << numpy==1.25.2 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,012][PROCESS][INFO]: >>> Ignored installing << pandas==1.5.3 >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,013][PROCESS][INFO]: >>> Ignored installing << scikit-learn >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,013][PROCESS][INFO]: >>> Ignored installing << matplotlib >>. Another version would be installed in the previous step.\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,014][PROCESS][INFO]: ======================================== Start dependency installation : << input >> \u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,015][PROCESS][INFO]: Start checking existence & installing package - pandas==1.5.3 | Progress: ( 1 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,016][PROCESS][INFO]: [OK] << pandas==1.5.3 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,016][PROCESS][INFO]: ======================================== Start dependency installation : << graph >> \u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,017][PROCESS][INFO]: Start checking existence & installing package - torch==2.0.0 | Progress: ( 2 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,018][PROCESS][INFO]: [OK] << torch==2.0.0 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,018][PROCESS][INFO]: Start checking existence & installing package - torchbiggraph@git+https://github.com/facebookresearch/PyTorch-BigGraph.git | Progress: ( 3 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,019][PROCESS][INFO]: [OK] << torchbiggraph@git+https://github.com/facebookresearch/PyTorch-BigGraph.git >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,020][PROCESS][INFO]: ======================================== Start dependency installation : << preprocess >> \u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,020][PROCESS][INFO]: Start checking existence & installing package - category_encoders | Progress: ( 4 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,021][PROCESS][INFO]: [OK] << category_encoders >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,021][PROCESS][INFO]: ======================================== Start dependency installation : << sampling >> \u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,022][PROCESS][INFO]: Start checking existence & installing package - numpy==1.25.2 | Progress: ( 5 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,023][PROCESS][INFO]: [OK] << numpy==1.25.2 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,024][PROCESS][INFO]: Start checking existence & installing package - numba==0.58.0 | Progress: ( 6 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,024][PROCESS][INFO]: [OK] << numba==0.58.0 >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,025][PROCESS][INFO]: Start checking existence & installing package - scikit-learn | Progress: ( 7 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,026][PROCESS][INFO]: [OK] << scikit-learn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,026][PROCESS][INFO]: Start checking existence & installing package - umap-learn | Progress: ( 8 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,027][PROCESS][INFO]: [OK] << umap-learn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,028][PROCESS][INFO]: Start checking existence & installing package - matplotlib | Progress: ( 9 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,028][PROCESS][INFO]: [OK] << matplotlib >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,029][PROCESS][INFO]: ======================================== Start dependency installation : << train >> \u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,030][PROCESS][INFO]: Start checking existence & installing package - seaborn | Progress: ( 10 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,030][PROCESS][INFO]: [OK] << seaborn >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,031][PROCESS][INFO]: Start checking existence & installing package - shap | Progress: ( 11 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,032][PROCESS][INFO]: [OK] << shap >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,032][PROCESS][INFO]: Start checking existence & installing package - lightgbm | Progress: ( 12 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,033][PROCESS][INFO]: [OK] << lightgbm >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,033][PROCESS][INFO]: Start checking existence & installing package - catboost | Progress: ( 13 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,034][PROCESS][INFO]: [OK] << catboost >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,035][PROCESS][INFO]: Start checking existence & installing package - ngboost | Progress: ( 14 / 15 total packages ) \u001b[0m\n",
      "\u001b[92m[2023-11-29 04:12:59,036][PROCESS][INFO]: [OK] << ngboost >> already exists\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,036][PROCESS][INFO]: ======================================== Start dependency installation : << force-reinstall >> \u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,037][PROCESS][INFO]: Start checking existence & installing package - numpy==1.25.2 --force-reinstall | Progress: ( 15 / 15 total packages ) \u001b[0m\n",
      "\u001b[94m[2023-11-29 04:12:59,037][PROCESS][INFO]: >>> Start installing package - numpy==1.25.2 --force-reinstall\u001b[0m\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "Successfully installed numpy-1.25.2\n",
      "\u001b[94m[2023-11-29 04:13:02,400][PROCESS][INFO]: ======================================== Finish dependency installation \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 사용하는 pipeline의 package를 설치\n",
    "# train = 0, infernence = 1을 선택해야 하고 둘다 설치 해야함\n",
    "pipeline = pipelines[0]\n",
    "alo.install_steps(pipeline, alo.control[\"get_asset_source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 data structure 구성\n",
    "alo.set_asset_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(step, pipeline, asset_structure):\n",
    "    # 반복되는 작업을 함수로 변환\n",
    "    asset_config = alo.asset_source[pipeline]\n",
    "    return alo.process_asset_step(asset_config[step], step, pipeline, asset_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 진행할 step을 list형식으로 입력하기\n",
    "step = 0\n",
    "# 입력할 args를 변수로 저장\n",
    "alo.asset_structure.args = alo.get_args(pipeline, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 사용자가 args를 변경해서 사용 가능\n",
    "# input_args[0]['input_path'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-29 13:13:48,179][USER][INFO][train_pipeline][input]: >> Load path : ['/home/ws.jang/alo_2.1/alo_2.1/gcr/alo/input/train/sample/']\n",
      "[2023-11-29 13:13:48,186][USER][INFO][train_pipeline][input]: >> The file for batch data has been loaded. (File name: /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/input/train/sample/customers.csv)\n",
      "[2023-11-29 13:13:48,188][USER][INFO][train_pipeline][input]: You set the << use_all_x >> as << True >> in the yaml file. So skip checking dataframe columns existence.\n",
      "[2023-11-29 13:13:48,190][USER][INFO][train_pipeline][input]: ==================== Success loading dataframe ====================\n",
      "[2023-11-29 13:13:48,192][USER][INFO][train_pipeline][input]: >> Start processing ignore columns & drop columns: ['/home/ws.jang/alo_2.1/alo_2.1/gcr/alo/input/train/sample/customers.csv']\n",
      "[2023-11-29 13:13:48,194][USER][INFO][train_pipeline][input]: >> You set the << use_all_x >> parameter as << True >> in your config yaml. (So, these x_columns are used: ['job', 'hobbies', 'FLAG_TRAIN_INFERENCE', 'address', 'orders', 'spent', 'name', 'age', 'gender'] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[2023-11-29 04:13:48,178][ASSET][INFO][train_pipeline][input]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-29 04:13:48\n",
      "- current step      : input\n",
      "- asset branch.     : tabular_2.0\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path'])\n",
      "- load args. keys   : dict_keys(['input_path', 'x_columns', 'use_all_x', 'y_column', 'groupkey_columns', 'drop_columns', 'time_column', 'concat_dataframes', 'encoding'])\n",
      "- load config. keys : dict_keys(['meta'])\n",
      "- load data keys    : dict_keys([])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:13:48,195][ASSET][INFO][train_pipeline][input]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-29 04:13:48\n",
      "- current step      : input\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:13:48,196][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: input\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "alo.asset_structure = run(step, pipeline, alo.asset_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1 \n",
    "alo.asset_structure.args = alo.user_parameters[pipeline][step]['args'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[2023-11-29 04:14:45,199][ASSET][INFO][train_pipeline][graph]: Successfully got << output path >> for saving your data into csv or jpg file: \n",
      " /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/ \n",
      " - [NOTE] The names of output file must be fixed as << output.csv, output.jpg >> \u001b[0m\n",
      "\u001b[94m[2023-11-29 04:14:45,201][ASSET][INFO][train_pipeline][graph]: \n",
      "\n",
      "============================= ASSET START =============================\n",
      "- time (UTC)        : 2023-11-29 04:14:45\n",
      "- current step      : graph\n",
      "- asset branch.     : release-1.2\n",
      "- alolib ver.       : 2.1\n",
      "- alo ver.          : 2.1\n",
      "- load envs. keys   : dict_keys(['project_home', 'solution_metadata_version', 'artifacts', 'alo_version', 'interface_mode', 'proc_start_time', 'save_train_artifacts_path', 'save_inference_artifacts_path', 'pipeline', 'step', 'num_step', 'asset_branch', 'load_data', 'load_config', 'save_data', 'save_config', 'log_file_path', 'prev_step'])\n",
      "- load args. keys   : dict_keys(['graph_type', 'center_node_column', 'embedding_column', 'train_inference_column', 'drop_columns', 'dimension', 'num_epochs', 'workers', 'num_partitions', 'extra_columns_for_ml', 'custom_connection'])\n",
      "- load config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- load data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "preprocessing blank space...\n",
      "In __init__: pbg ready\n",
      "[2023-11-29 13:14:45.366618] Using the 7 relation types given in the config\n",
      "[2023-11-29 13:14:45.367586] Searching for the entities in the edge files...\n",
      "[2023-11-29 13:14:45.378103] Entity type address:\n",
      "[2023-11-29 13:14:45.378849] - Found 886 entities\n",
      "[2023-11-29 13:14:45.379536] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-29 13:14:45.380283] - Left with 886 entities\n",
      "[2023-11-29 13:14:45.380927] - Shuffling them...\n",
      "[2023-11-29 13:14:45.382084] Entity type name:\n",
      "[2023-11-29 13:14:45.382791] - Found 985 entities\n",
      "[2023-11-29 13:14:45.383450] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-29 13:14:45.384180] - Left with 985 entities\n",
      "[2023-11-29 13:14:45.384813] - Shuffling them...\n",
      "[2023-11-29 13:14:45.386090] Entity type age:\n",
      "[2023-11-29 13:14:45.386731] - Found 63 entities\n",
      "[2023-11-29 13:14:45.387350] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-29 13:14:45.388003] - Left with 63 entities\n",
      "[2023-11-29 13:14:45.388615] - Shuffling them...\n",
      "[2023-11-29 13:14:45.389277] Entity type job:\n",
      "[2023-11-29 13:14:45.389930] - Found 37 entities\n",
      "[2023-11-29 13:14:45.390416] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-29 13:14:45.390898] - Left with 37 entities\n",
      "[2023-11-29 13:14:45.391342] - Shuffling them...\n",
      "[2023-11-29 13:14:45.391828] Entity type hobbies:\n",
      "[2023-11-29 13:14:45.392276] - Found 27 entities\n",
      "[2023-11-29 13:14:45.392720] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-29 13:14:45.393193] - Left with 27 entities\n",
      "[2023-11-29 13:14:45.393632] - Shuffling them...\n",
      "[2023-11-29 13:14:45.394098] Entity type orders:\n",
      "[2023-11-29 13:14:45.394558] - Found 21 entities\n",
      "[2023-11-29 13:14:45.395011] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-29 13:14:45.395475] - Left with 21 entities\n",
      "[2023-11-29 13:14:45.395952] - Shuffling them...\n",
      "[2023-11-29 13:14:45.396433] Entity type gender:\n",
      "[2023-11-29 13:14:45.396901] - Found 2 entities\n",
      "[2023-11-29 13:14:45.397376] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-29 13:14:45.397853] - Left with 2 entities\n",
      "[2023-11-29 13:14:45.398308] - Shuffling them...\n",
      "[2023-11-29 13:14:45.398811] Entity type spent:\n",
      "[2023-11-29 13:14:45.399292] - Found 322 entities\n",
      "[2023-11-29 13:14:45.399742] - Removing the ones with fewer than 1 occurrences...\n",
      "[2023-11-29 13:14:45.400290] - Left with 322 entities\n",
      "[2023-11-29 13:14:45.400680] - Shuffling them...\n",
      "[2023-11-29 13:14:45.401413] Preparing counts and dictionaries for entities and relation types:\n",
      "[2023-11-29 13:14:45.407697] - Writing count of entity type address and partition 0\n",
      "[2023-11-29 13:14:45.409093] - Writing count of entity type name and partition 0\n",
      "[2023-11-29 13:14:45.410566] - Writing count of entity type age and partition 0\n",
      "[2023-11-29 13:14:45.411497] - Writing count of entity type job and partition 0\n",
      "[2023-11-29 13:14:45.412402] - Writing count of entity type hobbies and partition 0\n",
      "[2023-11-29 13:14:45.413294] - Writing count of entity type orders and partition 0\n",
      "[2023-11-29 13:14:45.414191] - Writing count of entity type gender and partition 0\n",
      "[2023-11-29 13:14:45.415071] - Writing count of entity type spent and partition 0\n",
      "[2023-11-29 13:14:45.416220] Preparing edge path /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_2, out of the edges found in /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/tsvs/rel_2.tsv\n",
      "[2023-11-29 13:14:45.416914] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-29 13:14:45.603523] - Processed 890 edges in total\n",
      "[2023-11-29 13:14:45.604793] Preparing edge path /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_7, out of the edges found in /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/tsvs/rel_7.tsv\n",
      "[2023-11-29 13:14:45.605661] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-29 13:14:45.768209] - Processed 883 edges in total\n",
      "[2023-11-29 13:14:45.769768] Preparing edge path /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_5, out of the edges found in /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/tsvs/rel_5.tsv\n",
      "[2023-11-29 13:14:45.770460] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-29 13:14:45.933221] - Processed 886 edges in total\n",
      "[2023-11-29 13:14:45.934521] Preparing edge path /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_8, out of the edges found in /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/tsvs/rel_8.tsv\n",
      "[2023-11-29 13:14:45.935357] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-29 13:14:46.097550] - Processed 885 edges in total\n",
      "[2023-11-29 13:14:46.098987] Preparing edge path /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_3, out of the edges found in /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/tsvs/rel_3.tsv\n",
      "[2023-11-29 13:14:46.099714] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-29 13:14:46.264356] - Processed 884 edges in total\n",
      "[2023-11-29 13:14:46.265665] Preparing edge path /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_1, out of the edges found in /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/tsvs/rel_1.tsv\n",
      "[2023-11-29 13:14:46.266470] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-29 13:14:46.428281] - Processed 886 edges in total\n",
      "[2023-11-29 13:14:46.429611] Preparing edge path /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/partitions/edges_partitioned_rel_4, out of the edges found in /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/tsvs/rel_4.tsv\n",
      "[2023-11-29 13:14:46.430414] - Edges will be partitioned in 1 x 1 buckets.\n",
      "[2023-11-29 13:14:46.591264] - Processed 884 edges in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws.jang/miniforge3/envs/alo_2.1/lib/python3.10/site-packages/torchbiggraph/util.py:222: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = tensor.storage_type()._new_shared(size.numel())\n",
      "/home/ws.jang/miniforge3/envs/alo_2.1/lib/python3.10/site-packages/torch/storage.py:959: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if self.device.type not in ['cpu', 'cuda']:\n",
      "/home/ws.jang/miniforge3/envs/alo_2.1/lib/python3.10/site-packages/torch/storage.py:962: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  module = torch if self.device.type == 'cpu' else torch.cuda\n",
      "/home/ws.jang/miniforge3/envs/alo_2.1/lib/python3.10/site-packages/torch/storage.py:985: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  untyped_storage = torch.UntypedStorage._new_shared(size * cls()._element_size())\n",
      "/home/ws.jang/miniforge3/envs/alo_2.1/lib/python3.10/site-packages/torch/storage.py:986: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return cls(wrap_storage=untyped_storage)\n",
      "/home/ws.jang/miniforge3/envs/alo_2.1/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/ws.jang/miniforge3/envs/alo_2.1/lib/python3.10/site-packages/torchbiggraph/train_cpu.py:304: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ).storage()\n",
      "/home/ws.jang/miniforge3/envs/alo_2.1/lib/python3.10/site-packages/torchbiggraph/train_cpu.py:821: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  self.embedding_storage_freelist[entity].add(embs.storage())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embedding Complete]\n",
      "[Embeddig result saved at /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/output/graph/RESULT]\n",
      "\u001b[92m[2023-11-29 04:15:05,506][ASSET][INFO][train_pipeline][graph]: Successfully got model path for saving or loading your AI model: \n",
      " /home/ws.jang/alo_2.1/alo_2.1/gcr/alo/.train_artifacts/models/graph/\u001b[0m\n",
      "In __del__: pbg deleted\n",
      "\u001b[94m[2023-11-29 04:15:05,538][ASSET][INFO][train_pipeline][graph]: \n",
      "\n",
      "============================= ASSET FINISH ===========================\n",
      "- time (UTC)        : 2023-11-29 04:15:05\n",
      "- current step      : graph\n",
      "- save config. keys : dict_keys(['meta', 'data_source_type', 'time_format', 'time_column', 'x_columns', 'input_path', 'group_cnt', 'group_keys', 'y_column', 'input_asset_df_path', 'ignore_columns'])\n",
      "- save data keys    : dict_keys(['dataframe'])\n",
      "=======================================================================\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[94m[2023-11-29 04:15:05,538][PROCESS][INFO]: ==================== Finish pipeline: train_pipeline / step: graph\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "alo.asset_structure = run(step, pipeline, alo.asset_structure)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이후 step 들도 2, 3.. 늘려가며 동일하게 실행 가능 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alo_2.1",
   "language": "python",
   "name": "alo_2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c44a7ce709843662cd524fcc2a8e8e0bd459ed9a4e30f104c2fe1b493331e35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
