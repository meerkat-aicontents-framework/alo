## 외부에서 데이터 가져오기 / 결과 저장하는 경우 해당 위치에 지정
external_path:
    - load_train_data_path: /nas001/projects/aicontents_forecast/fcst_samples/train_input_path/
    - load_inference_data_path: /nas001/projects/aicontents_forecast/fcst_samples/inference_input_path/
    - save_train_artifacts_path:
    - save_inference_artifacts_path:
    - load_s3_key_path: /nas001/users/woosung.jang/aws_key

external_path_permission:
    - s3_private_key_file:

## 실험에 필요한 파라미터를 설정함 
## - 해당 위치에서 삭제되면, code 의 default 로 실행
user_parameters:
    - train_pipeline:
        - step: input  ## 필수
          args:
            - input_path: train_input_path
              x_columns: 
              use_all_x: True
              y_column: pollution
              groupkey_columns:
              drop_columns:
              time_column: date
        - step: train ## 필수
          args:
            - input_path: train_input_path
              model_path: ${env(project_home)}/samples/fcst/models/
              output_path: ${env(project_home)}/samples/fcst/outputs/
              input_file: AirPollution_2m.csv
              ## Input parameters - Mandatory
              # Mandatory
              forecaster_name: nbeats  # gam_stat|nbeats
              sample_frequency: hourly
              forecasting_periods: 72
              country_name: CN  # DE, CN, KR, and so on
              group_key_variates: ['wnd_dir']
              multi_learn: False  # True|False
              time_index_variate: date
              y_variate: pollution
              x_covariates: ['dew','temp','press','wnd_spd','snow','rain']
              # Optional
              static_covariates: []
              holiday_covariates: []
              inference_sample_length: None # Sample lengths for inference sets. None or an integer.
              time_index_cutoff: None
              ## Hyper parameters - Optional. It will be determined automaticlly.
              ## 1) gam_stat hyper params to be HPO
              # Regulation method. > 0. The higher it is, the less regulation. Default = 10 (very less regulation). 
              # 10~20 will work.
              holidays_prior_scale: 10
              holidays_prior_scale_x_covariates: 10
              # fit_*_seasonality setting here (in yaml) affects the selection of x_covariates.
              # Then, it will be set during HPO.
              fit_yearly_seasonality: False
              fit_quarterly_seasonality: False
              fit_monthly_seasonality: False
              fit_weekly_seasonality: False
              fit_daily_seasonality: False
              fit_hourly_seasonality: False
              fit_custom_seasonality: False
              period_custom_seasonality: 1/(24*6)
              # > 0. The higher it is, the better it captures high variance. Default = 10.
              fourier_order: 10
              fourier_order_x_covariates: 10
              # Regulation method. > 0. The higher it is, the less regulation. Default = 10 (very less regulation). 
              # 10~25 will work.
              prior_scale: 10
              prior_scale_x_covariates: 10
              # How to compose the furier functions for each seasonality. 
              # Either 'multiplicative' or 'additive'. Default = 'additive
              seasonality_mode: multiplicative
              seasonality_mode_x_covariates: multiplicative
              # 0 <= mcmc_samples <= the entire sample number. Default = 0 (interval only for trend).
              # Set to a value greater than 0 in order to get the uncertainty interval in report for seasonalities.
              mcmc_samples: 0
              mcmc_samples_x_covariates: 0
              # Allowed Uncertainty interval width for trend. 0.0 < interval_width < 1.0, Default == 0.8. 
              # 0.9~0.99 will work.
              interval_width: 0.8
              interval_width_x_covariates: 0.8
              # User specific changepoints like ['2023-01-01', '2023-07-01']. Default = None
              changepoints: None
              changepoints_x_covariates: None
              # Number of changepoints for automatic changepoint detection. Default = 25
              n_changepoints: 25
              n_changepoints_x_covariates: 25
              # Default = 0.05. Increasing it will make the trend more flexible. 0.4~0.5 will work.
              changepoint_prior_scale: 0.05
              changepoint_prior_scale_x_covariates: 0.05
              # The portion of input time series to observe the changepoints automatically. Default = 0.8
              changepoint_range: 0.8
              changepoint_range_x_covariates: 0.8
              # Trend's growth method. 'logistic' or 'linear'. Default = 'linear'.
              growth: logistic
              ## 2) nbeats hyper params to be HPO
              # (int). = M. Works only for generic arch. Should be >= 30 for generic arch, but 2 for interpretable arch.
              nbeats_num_stacks: 30
              # (int). = K. Number of blocks within a stack.
              nbeats_num_blocks: 3
              # (int). = J. Number of FC layers prior to the forking layers.
              nbeats_num_layers: 4
              # (list of K ints (individual ints for each Block). Or an int (same for all K Blocks)). Output dim of FC.
              nbeats_layer_widths: 128
              # (int). Works only for generic arch. Output dim of g^b and g^f (expansion coef).
              nbeats_expansion_coefficient_dim: 128
              # (int). Works only for interpretable arch. Number of linear lines that composes the trend.
              nbeats_trend_polynomial_degree: 4
              # (float). Dropout rate for a FC layer. Works only when mc_dropout = True at the prediction time.
              nbeats_dropout: 0.5
              ## 3) Other hyper params to control the operation.
              # ---------- nbeats specific hyper params -----------
              # (int). Model's input time sequence length. .
              nbeats_input_chunk_length: 1
              # (int). Model's forecasting time sequence length.
              nbeats_output_chunk_length: 1
              nbeats_generic_architecture: True  # (bool). True, False
              # (str). Activation function. ReLU (default), RReLU, PReLU, Softplus, Tanh, SELU, LeakyReLU, Sigmoid.
              nbeats_activation: ReLU
              # Number of time series samples for each training. Default = 32.
              nbeats_batch_size: 32
              # Number of iterative training on a same set of samples. Default = 100.
              nbeats_n_epochs: 30
              # ---------- common to nbeats and gam_stat hyper params -----------
              n_group_key_variates: 0
              list_n_group_key_uval: []
              n_group_key_compositions: 1
              group_key_compositions: [[]]
              order_group_key_compositions: 0
              # If False, the x_covariates given by user (in form of x_covariates=['a','b','c']) 
              # will be used without HPO_x_covariates.
              optimize_x_covariates: True
              #optimize_x_covariates=False
              # If False, the hyper parameters in 2.1) will be used without HPO.
              optimize_parameters: True
              #optimize_parameters=False
              # True: testset_size is number. False: testset_size is ratio
              testset_by_nr: True
              # If testset_by_nr is True, testset_size is the sample number.
              # Otherwise, it is the ratio to the entire input data.
              testset_size: 0
              # True: validation_set_size is number. False: validation_set_size is ratio
              validation_set_by_nr: True
              # If validation_set_by_nr is True, validation_set_size is the sample number.
              # Otherwise, it is the ratio to the entire train_set (i.e., the entire input data - testset).
              validation_set_size: 0
              # Kfolds - The validation set and test set will be further divided into kfolds sections.
              # Forcasting will be made for each section, and the overall evaluation will be made over the all sections.
              kfolds_cv: 3
              kfolds_test: 1
              # The threshold on the sum of x_covariates' normalized feature importances in descending order.
              # 0 <= value <= 1. Default = 0.7.
              x_covariate_coef_threshold: 0.7
              # Default = MASE. Can be set to RMSE, RMSE%, MAE, MAE%, MAPE, SMAPE, MASE.
              # But, considering the possible output value ranges, RMSE%, MAE%, and MASE are recommended.
              metric_to_compare: MASE
              # RMSE%.MAE%. 0 < threshold < inf. Default = 100, MASE. 0 < threshold < inf. Default = 1.0
              x_covariate_error_threshold: 1.0
              # Acquisition function of the Baysian optimization for HPO.
              # One of LCB, EI, PI, gp_hedge, EIps, PIps. Default = EI.
              baysian_opt_acq_func: EI
              # Baysian optimization's acquision function tries n_calls times to find the global optima point.
              n_calls: 20
              # tsfresh's fc_param value. 'minimal' or 'comprehensive'. Default = 'minimal
              tsf_param: minimal
              # Default = 0.001. Outlier screening threshold for Isolated Forest.
              iso_contamination_threshold: 0.001
              # Upper bound of forecasted Y (cap) will be the max of input data x cap_ratio.
              cap_ratio: 1.5
              # Train model using multiple time series
              multivariates_learning: False
              report_train_diagnosis: True
              report_forecasting_results: True
              plot_forecast: True
              alg_ver: v1.0
              step: train

    - inference_pipeline:
        - step: input  ## 필수
          args:
            - input_path: inference_input_path
              x_columns: 
              use_all_x: True
              y_column: pollution
              groupkey_columns:
              drop_columns:
              time_column: date

        - step: inference ## 필수
          args:
            - input_path: inference_input_path
              model_path: ${env(project_home)}/samples/fcst/models/
              output_path: ${env(project_home)}/samples/fcst/outputs/
              input_file: AirPollution_2w.csv
              ## Input parameters - Mandatory
              # Mandatory
              forecaster_name: nbeats  # gam_stat|nbeats
              sample_frequency: hourly
              forecasting_periods: 72
              country_name: CN  # DE, CN, KR, and so on
              group_key_variates: ['wnd_dir']
              multi_learn: False  # True|False
              time_index_variate: date
              y_variate: pollution
              x_covariates: ['dew','temp','press','wnd_spd','snow','rain']
              # Optional
              static_covariates: []
              holiday_covariates: []
              inference_sample_length: None # Sample lengths for inference sets. None or an integer.
              time_index_cutoff: None
              ## Hyper parameters - Optional. It will be determined automaticlly.
              ## 1) gam_stat hyper params to be HPO
              # Regulation method. > 0. The higher it is, the less regulation. Default = 10 (very less regulation). 
              # 10~20 will work.
              holidays_prior_scale: 10
              holidays_prior_scale_x_covariates: 10
              # fit_*_seasonality setting here (in yaml) affects the selection of x_covariates.
              # Then, it will be set during HPO.
              fit_yearly_seasonality: False
              fit_quarterly_seasonality: False
              fit_monthly_seasonality: False
              fit_weekly_seasonality: False
              fit_daily_seasonality: False
              fit_hourly_seasonality: False
              fit_custom_seasonality: False
              period_custom_seasonality: 1/(24*6)
              # > 0. The higher it is, the better it captures high variance. Default = 10.
              fourier_order: 10
              fourier_order_x_covariates: 10
              # Regulation method. > 0. The higher it is, the less regulation. Default = 10 (very less regulation). 
              # 10~25 will work.
              prior_scale: 10
              prior_scale_x_covariates: 10
              # How to compose the furier functions for each seasonality. 
              # Either 'multiplicative' or 'additive'. Default = 'additive
              seasonality_mode: multiplicative
              seasonality_mode_x_covariates: multiplicative
              # 0 <= mcmc_samples <= the entire sample number. Default = 0 (interval only for trend).
              # Set to a value greater than 0 in order to get the uncertainty interval in report for seasonalities.
              mcmc_samples: 0
              mcmc_samples_x_covariates: 0
              # Allowed Uncertainty interval width for trend. 0.0 < interval_width < 1.0, Default == 0.8. 
              # 0.9~0.99 will work.
              interval_width: 0.8
              interval_width_x_covariates: 0.8
              # User specific changepoints like ['2023-01-01', '2023-07-01']. Default = None
              changepoints: None
              changepoints_x_covariates: None
              # Number of changepoints for automatic changepoint detection. Default = 25
              n_changepoints: 25
              n_changepoints_x_covariates: 25
              # Default = 0.05. Increasing it will make the trend more flexible. 0.4~0.5 will work.
              changepoint_prior_scale: 0.05
              changepoint_prior_scale_x_covariates: 0.05
              # The portion of input time series to observe the changepoints automatically. Default = 0.8
              changepoint_range: 0.8
              changepoint_range_x_covariates: 0.8
              # Trend's growth method. 'logistic' or 'linear'. Default = 'linear'.
              growth: logistic
              ## 2) nbeats hyper params to be HPO
              # (int). = M. Works only for generic arch. Should be >= 30 for generic arch, but 2 for interpretable arch.
              nbeats_num_stacks: 30
              # (int). = K. Number of blocks within a stack.
              nbeats_num_blocks: 3
              # (int). = J. Number of FC layers prior to the forking layers.
              nbeats_num_layers: 4
              # (list of K ints (individual ints for each Block). Or an int (same for all K Blocks)). Output dim of FC.
              nbeats_layer_widths: 128
              # (int). Works only for generic arch. Output dim of g^b and g^f (expansion coef).
              nbeats_expansion_coefficient_dim: 128
              # (int). Works only for interpretable arch. Number of linear lines that composes the trend.
              nbeats_trend_polynomial_degree: 4
              # (float). Dropout rate for a FC layer. Works only when mc_dropout = True at the prediction time.
              nbeats_dropout: 0.5
              ## 3) Other hyper params to control the operation.
              # ---------- nbeats specific hyper params -----------
              # (int). Model's input time sequence length. .
              nbeats_input_chunk_length: 1
              # (int). Model's forecasting time sequence length.
              nbeats_output_chunk_length: 1
              nbeats_generic_architecture: True  # (bool). True, False
              # (str). Activation function. ReLU (default), RReLU, PReLU, Softplus, Tanh, SELU, LeakyReLU, Sigmoid.
              nbeats_activation: ReLU
              # Number of time series samples for each training. Default = 32.
              nbeats_batch_size: 32
              # Number of iterative training on a same set of samples. Default = 100.
              nbeats_n_epochs: 30
              # ---------- common to nbeats and gam_stat hyper params -----------
              n_group_key_variates: 0
              list_n_group_key_uval: []
              n_group_key_compositions: 1
              group_key_compositions: [[]]
              order_group_key_compositions: 0
              # If False, the x_covariates given by user (in form of x_covariates=['a','b','c']) 
              # will be used without HPO_x_covariates.
              optimize_x_covariates: True
              #optimize_x_covariates=False
              # If False, the hyper parameters in 2.1) will be used without HPO.
              optimize_parameters: True
              #optimize_parameters=False
              # True: testset_size is number. False: testset_size is ratio
              testset_by_nr: True
              # If testset_by_nr is True, testset_size is the sample number.
              # Otherwise, it is the ratio to the entire input data.
              testset_size: 0
              # True: validation_set_size is number. False: validation_set_size is ratio
              validation_set_by_nr: True
              # If validation_set_by_nr is True, validation_set_size is the sample number.
              # Otherwise, it is the ratio to the entire train_set (i.e., the entire input data - testset).
              validation_set_size: 0
              # Kfolds - The validation set and test set will be further divided into kfolds sections.
              # Forcasting will be made for each section, and the overall evaluation will be made over the all sections.
              kfolds_cv: 3
              kfolds_test: 1
              # The threshold on the sum of x_covariates' normalized feature importances in descending order.
              # 0 <= value <= 1. Default = 0.7.
              x_covariate_coef_threshold: 0.7
              # Default = MASE. Can be set to RMSE, RMSE%, MAE, MAE%, MAPE, SMAPE, MASE.
              # But, considering the possible output value ranges, RMSE%, MAE%, and MASE are recommended.
              metric_to_compare: MASE
              # RMSE%.MAE%. 0 < threshold < inf. Default = 100, MASE. 0 < threshold < inf. Default = 1.0
              x_covariate_error_threshold: 1.0
              # Acquisition function of the Baysian optimization for HPO.
              # One of LCB, EI, PI, gp_hedge, EIps, PIps. Default = EI.
              baysian_opt_acq_func: EI
              # Baysian optimization's acquision function tries n_calls times to find the global optima point.
              n_calls: 20
              # tsfresh's fc_param value. 'minimal' or 'comprehensive'. Default = 'minimal
              tsf_param: minimal
              # Default = 0.001. Outlier screening threshold for Isolated Forest.
              iso_contamination_threshold: 0.001
              # Upper bound of forecasted Y (cap) will be the max of input data x cap_ratio.
              cap_ratio: 1.5
              # Train model using multiple time series
              multivariates_learning: False
              report_train_diagnosis: True
              report_forecasting_results: True
              plot_forecast: True
              alg_ver: v1.0
              step: inference

        - step: post_analysis
          args:
            - input_path: inference_input_path
              create_result_report: True
              x_columns: 
              sample_frequency: daily
              ds_feature: date
              y_feature: y
              forecasting_periods: 90
              forecaster_name: gam_stat
            
## asset 의 설치 정보를 기록       
asset_source:
    - train_pipeline:
        - step: input
          source:  ## git / local 지원
            code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/input.git
            # code: local
            branch: tabular_2.0 ##previous setting : tabular_dev
            requirements:
              - pandas==1.5.3
        - step: train
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/forecast.git
            # code: local
            branch: release-1.2
            requirements:
              - requirements.txt
              #- scikit-optimize==0.9.0
              #- numpy==1.21.6 --force-reinstall 
              #- numba==0.55.0 --force-reinstall

    - inference_pipeline:
        - step: input
          source:  ## git / local 지원
            code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/input.git
            # code: local
            branch: tabular_2.0 ##previous setting : tabular_dev
            requirements:
              - pandas>=1.5.3
        
        - step: inference
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/forecast.git
            # code: local
            branch: release-1.2
            requirements:
              - pandas>=1.5.3

        - step: post_analysis
          source:
            code: http://mod.lge.com/hub/dxadvtech/assets/forecast.git
            # code: local
            branch: release-1.2
            requirements:
              - pandas>=1.5.3

control:
  ## 1. 패키지 설치 및 asset 존재 여부를 실험 시마다 체크할지, 한번만 할지 결정
  ## 1-2 requirements.txt 및 종속 패키지들 한번만 설치할 지 매번 설치할지도 결정 
  - get_asset_source: once ## once, every
  # TODO 아래 get_external_data 제작하기
  - get_external_data: once ## once, every
  ## 2. 생성된 artifacts 를 backup 할지를 결정 True/False
  - backup_artifacts: True
  ## 3. pipeline 로그를 backup 할지를 결정 True/False
  - backup_log: True
  ## 4. 저장 공간 사이즈를 결정 (단위 MB)
  - backup_size: 1000

  ## 5. Asset 사이 데이터 전달 방법으로 memory, file 를 지원
  - interface_mode: memory

    # ## 1. 패키지 설치 및 asset 존재 여부를 실험 시마다 체크할지, 한번만 할지 결정
    # - check_asset_source: once ## once, every
    # ## 2. 생성된 artifacts 를 backup 할지를 결정 True/False
    # - backup_artifacts: True
    # ## 3. pipeline 로그를 backup 할지를 결정 True/False
    # - backup_log: True
    # ## 4. 저장 공간 사이즈를 결정 (단위 MB)
    # - backup_size: 1000
 
    # ## 5. Asset 사이 데이터 전달 방법으로 memory, file 를 지원
    # - interface_mode: memory
